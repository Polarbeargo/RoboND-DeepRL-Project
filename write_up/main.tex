%% V1.0
%% by Gabriel Garcia, gabrcg@gmail.com
%% This is a template for Udacity projects using IEEEtran.cls

%% Be Udacious!

\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[pdftex]{graphicx}    
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\begin{document}

\title{Deep RL Arm Manipulation}

\author{Hsin-Wen Chang}

\markboth{Deep Reinforcement Learning Arm Manipulation, DQN, Robotics Software Engineer Nanodegree Program, Udacity}%
{}
\IEEEtitleabstractindextext{%

\begin{abstract}
The goal of this project is to create a DQN agent and define reward functions to teach a robotic arm to perform two primary objectives. one is aim to have any part of the robot arm touch the object of interest with at least a 90 percent accuracy. Second is aim for only the gripper base of the robot arm touch the object, with at least a 80 percent accuracy.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Deep Reinforcement Learning Arm Manipulation, DQN, LSTM, Robotics Software Engineer Nanodegree Program, Robot, IEEEtran, Udacity, \LaTeX.
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle
\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{D}{ee}p Q-Network(DQN) introduced to training a Q value function represented by multi-layer perceptron. This project will use Gazebo simulation environment integrate with C/C++ API developed by NVIDIA to interface Gazebo robotic arm to DQN agent run simulation and training the LSTM based Deep Q-Network(DQN) on Udacity GPU enabled server with the following task will be implemented:  

- Subscribe to camera and collision topics.  

- Create the DQN Agent.

- Velocity or position based control of arm joints.

- Reward for robot gripper hitting the ground.

- Interim reward based on the distance to the object.

- Reward based on collision between the arm and the object.

- Tuning the hyperparameters.

- Reward based on collision between the armâ€™s gripper base and the object.
\section{Background}
  %example for inserting image
\begin{figure}[thpb]
      \centering
      \includegraphics[width=\linewidth]{FromRLToDeepRL.png}
      \caption{From RL to Deep RL.}
      \label{fig:robot1}
\end{figure}
The difference between RL and Deep RL is the use of a deep neural network. Think of the collection of value-action pairs that define what actions an agent should take in any situation as a function of the observations that the agent receives from its environment. Neural network can be used to approximate this function because through its large quantity of parameters that can be learned through trial and error.
\subsection{Setup and building from Source (Nvidia Jetson TX2)}

\begin{lstlisting}
$ sudo apt-get install cmake
$ conda install -c pytorch pytorch
$ sudo apt-get install libignition-math2-dev
$ git clone http://github.com/udacity/RoboND-DeepRL-Project
$ cd RoboND-DeepRL-Project
$ git submodule update --init
$ mkdir build
$ cd build
$ cmake ../
$ make
\end{lstlisting}
%example for Bullet point list
\begin{itemize}
\item  Subscribe to camera topics:
\begin{lstlisting}
//Subscribe to camera topic
cameraSub = cameraNode->Subscribe("/gazebo/arm_world
/camera/link/camera/image", &ArmPlugin::onCameraMsg, this);
\end{lstlisting}
\item Subscribe to collision topics:
\begin{lstlisting}
// Subscribe to prop collision topic
collisionSub = collisionNode->Subscribe("/gazebo
/arm_world/tube/tube_link/my_contact", &ArmPlugin::onCollisionMsg, this);
\end{lstlisting}
\item Create the DQN Agent:
\begin{lstlisting}
// Create DQN Agent
agent = dqnAgent::Create(INPUT_WIDTH, INPUT_HEIGHT, INPUT_CHANNELS, NUM_ACTIONS, OPTIMIZER, LEARNING_RATE, REPLAY_MEMORY, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY,USE_LSTM, LSTM_SIZE, ALLOW_RANDOM, DEBUG_DQN);
\end{lstlisting}
\item Velocity or position based control of arm joints:

The DQN output is mapped to a particular action such as the control of each joint for the robotic arm. In ArmPlugin::updateAgent(), there are two existing approaches to control the joint movements.
\begin{lstlisting}
const int jointIndex = action / 2;
const int actionDirection = 1 - 2 * (action % 2);
//Increase or decrease the joint velocity based on whether the action is even or odd. Set joint velocity based on whether action is even or odd.
const float velocity = vel[jointIndex] + actionVelDelta * actionDirection; 
// Increase or decrease the joint position based on whether the action is even or odd Set joint position based on whether action is even or odd.
float joint = ref[jointIndex] + actionJointDelta * actionDirection;
\end{lstlisting}

\item Reward for robot gripper hitting the ground:
\begin{lstlisting}
// Get the bounding box for the gripper		
const math::Box& gripBBox = gripper->GetBoundingBox();
const float groundContact = 0.05f;
// Set appropriate Reward for robot hitting the ground.
const bool checkGroundContact =  ( gripBBox.min.z <= groundContact || gripBBox.max.z <= groundContact );
\end{lstlisting}
\end {itemize}

\subsection{Reward function design}

\begin{lstlisting}
const float distDelta  = lastGoalDistance - distGoal;
// Compute the smoothed moving average of the delta of the distance to the goal
avgGoalDelta  = (avgGoalDelta * ALPHA) + (distDelta * (1.0f - ALPHA));
rewardHistory = (avgGoalDelta) * REWARD_MUL - PENALTY;
\end{lstlisting}

%example for Bullet point list
\begin{itemize}
\item REWARD LOSS: When robotic arm can't completed it's a round task, this penalty will be issued.
\item REWARD WIN: When robotic arm completed its round task, this reward will be issued.
\item REWARD MUL: Base on the distance between the arm and the object use this distance to calculate an appropriate reward.
\end {itemize}

\subsection{DQN agent's hyperparameters}
\begin{itemize}
\item Initial Parameters:
\begin{lstlisting}
#define INPUT_WIDTH   512
#define INPUT_HEIGHT  512
#define OPTIMIZER "None"
#define LEARNING_RATE 0.0f
#define REPLAY_MEMORY 10000
#define BATCH_SIZE 8
#define USE_LSTM false
#define LSTM_SIZE 32

\end{lstlisting}
\item Parameters Tuned:

Input image size is quite large and influence memory usage. Reduce image size will improve performance. Optimizer were changed to RMSprop which is commonly used and produce good result.Small number learning rate will slow learning speed but minimize error. Smaller batch size will reduce more computing power.
\begin{lstlisting}
#define INPUT_WIDTH   64 
#define INPUT_HEIGHT  64
#define OPTIMIZER "RMSprop" 
#define LEARNING_RATE 0.9f 
#define REPLAY_MEMORY 10000
#define BATCH_SIZE 128  
#define USE_LSTM true
#define LSTM_SIZE 256

\end{lstlisting}
\end {itemize}
\begin{table}[h]
 \begin{center}
      \begin{tabular}{ |c|c| } 
       \hline
       Hyperparameters & Values \\
       \hline
       INPUT WIDTH & 64 \\ 
       INPUT HEIGHT & 64 \\
       INPUT CHANNELS & 3\\
       NUM ACTIONS & DOF*2 \\
       OPTIMIZER & RMSprop \\
       LEARNING RATE & 0.9f\\
       REPLAY MEMORY & 10000 \\
       BATCH SIZE & 128\\
       GAMMA & 0.9f\\
       EPS START & 0.9f\\
       EPS END & 0.0f \\
       EPS DECAY & 250\\
       USE LSTM & true\\
       LSTM SIZE & 256\\
       ALLOW RANDOM & true\\
       DEBUG DQN & false \\
       \hline
      \end{tabular}
      \caption{Table of hyperparameters}
      \label{table:1}
      \end{center}
      \end{table}
\section{Results}
Both task have trained with the parameters in TABLE 1 and the two primary objectives were achieved:
\begin{itemize}
\item Have any part of the robot arm touch the object of interest, with at least a 90 percent accuracy. 
\begin{figure}[htpb]
      \centering
      \includegraphics[width=\linewidth]{90.png}
      \caption{Achieved 90 percent.}
      \label{fig:achieved 90 percent}
\end{figure}
\item Have only the gripper base of the robot arm touch the object, with at least a 80 percent accuracy.
\begin{figure}[htpb]
      \centering
      \includegraphics[width=\linewidth]{80.png}
      \caption{Achieved 80 percent}
      \label{fig:achieved 80 percent}
\end{figure}
\end {itemize}
As Observed if don't add time penalty the robotic arm will start trembly around it's target and if the LSTM size lower than 256 will take longer time to reach the target accuracy.

\section{Conclusion / Future work}
Training a Deep Reinforcement Learning agent takes a lot of time. Tuning LSTM size, image size, learning rate and batch size will influence the time reach it's target accuracy. The advantages of the C/C++ API develope by NVIDIA provide for robotics RL problems include leverage the power of GPU acceleration speed up RL agent training increase execution performance by using C/C++ compilation instead of interpreted python scripts optimized on specific embedded platform NVIDIA Jetson TX2. In the future exploration I would like to see the application of a genetic algorithm (GA) to find the values of parameters used in Deep Deterministic Policy Gradient (DDPG) with Hindsight Experience Replay (HER), to help speed up the learning agent[1] on NVIDIA Jetson TX2.

\bibliography{bib}
\bibliographystyle{ieeetr}

[1] Deep Reinforcement Learning using Genetic
Algorithm for Parameter Optimization
Adarsh Sehgal, Hung Manh La, Sushil J. Louis, Hai Nguyen. To Appear in arxiv 2019.
\end{document}
